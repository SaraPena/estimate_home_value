{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e5e9f865629e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msplit_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'features'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm, ttest_ind, pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from math import sqrt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "import env\n",
    "import acquire\n",
    "import prep\n",
    "import split_scale\n",
    "import features\n",
    "import model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "Deliverables:\n",
    "\n",
    "1. A report (in for the form of a presentaion, both verbal and through slide)\n",
    "\n",
    "Summarize your findings about the drivers of the Tax Value Count.\n",
    "This will come from the analysis you do during the exploration phase of the pipeline.\n",
    "In the report you will have charts that visually tell the story of what is driving the errors.\n",
    "\n",
    "2. A github repository containing the jupyter notebook that walks through the pipeline along with the .py files necessary to reproduce your model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Zillow Data Science Team Project Directive: \n",
    "Find qualities of properties that strongly influence the value of a home. Create a model that could be used to begin to predict a value of a home before an assesment is submitted. \n",
    "The data science team wants your model to have 95% a confidence interval. \n",
    "\n",
    "Which independent variables do you think influence the value of a home?\n",
    "\n",
    "Hypothesis:\n",
    "\n",
    "Variables that can influence the value of home are square feet,  and room count(bedroom, and bath). \n",
    "These two variables can have an influence no matter where the property located.\n",
    "\n",
    "The more square feet of a property the more the value of the home will rise.\n",
    "The higher the room count the cost will increase.\n",
    "The total square footage of the property will be a stronger influence than room count\n",
    "\n",
    "When you start to get into specific locations other factors can influence the value of a home such as zip code, school district, distance to destinations (work, home, restaurants, grocery).\n",
    "Also home amenities will influence the value of a property, such as backyards with pools, garage size, hoa.\n",
    "The age of home would also have influence.\n",
    "\n",
    "To begin our modeling we will look at the features of squarefootage and room count to see how these will influence the target variable - value of the property. Further exploration of other property, and location specific features can be explored after this first phase is built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***ACQUIRE***\n",
    "\n",
    "Goal: Gather and describe data from zillow dataset that creates a dataframe with:\n",
    "    Index : \n",
    "        parcelid,fips \n",
    "    Features:\n",
    "        calculatedfinishedsquarefeet\n",
    "        bathroomcnt\n",
    "        bedroomcnt\n",
    "        lotsizesquarefeet\n",
    "\n",
    "Include head of the dataset, datatypes, summary stats, shape of the dataframe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get url for zillow database using your env file. Your env file should include variables that are assigned to your username, password, and host for mysql.\n",
    "\n",
    "url = acquire.get_db_url('zillow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe for your features and target variable using your query, and url variable.\n",
    "# The features I was interested in exploring were bathroomcnt, bedroomcnt, calculatedfinishedsquarefeet, and lotsizesquarefeet. The target variable I want to predict is taxvaluedollarcnt\n",
    "\n",
    "query = \"\"\" SELECT parcelid,\n",
    "                   fips,\n",
    "                   bathroomcnt, \n",
    "                   bedroomcnt, \n",
    "                   calculatedfinishedsquarefeet, \n",
    "                   lotsizesquarefeet,\n",
    "                   taxvaluedollarcnt\n",
    "            FROM properties_2017\n",
    "            JOIN predictions_2017 using(parcelid)\n",
    "            WHERE (propertylandusetypeid = 261) and (transactiondate BETWEEN '2017-05-01' and '2017-06-30') \"\"\"\n",
    "# From the acuire.py create your dataframe with get_data_from_mysql(query,db)\n",
    "\n",
    "df = acquire.get_data_from_mysql(query,'zillow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Head of dataframe\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data types of dataframe:\n",
    "\n",
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of our data types are floats or integers which are types that we can measure and quatify. There is not a need at this point to convert the features to another data type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inital summary stats:\n",
    "\n",
    "initial_df = df.describe()\n",
    "initial_max = df.max()\n",
    "initial_min = df.min()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe columns:\n",
    "\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of dataframe:\n",
    "\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*PREP*\n",
    "\n",
    "Goal: \n",
    "    Create a dataset that is ready to be analyzed. \n",
    "    Datatypes are appropriate, null values, and integrity issues have been addressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find amount of nulls in dataframe:\n",
    "\n",
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 24 null values only in 'calculatedfinishedsquarefeet', and 124 Null values in 'lotsizesquarefeet'.  Because we have 15,034 rows in a our data frame we will drop out those values, because we believe that we have enough data to begin to make our models for predictions.\n",
    "# Use prep.py to clean the dataframe by dropping the rows with null values. As stated earlier because we are working with numeric data types we do not need to change the datatypes of any of our variables.\n",
    "\n",
    "clean_df = prep.clean_data(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at our descriptive stats to see if there any major changes\n",
    "\n",
    "clean_df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My preference is to have my index be my parcel_id, and fips number so that when I am splitting into train, test and scaling. I be won't be scaling my 'parcelid' , and 'fips' (identifies county of the property ) and the integrity of those columns will be intact.\n",
    "# Now that the data is clean I will rename it to be the dataframe - 'df'.\n",
    "\n",
    "df = clean_df .set_index(['parcelid','fips'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot the distributions of independent variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bathroomcnt box plots\n",
    "\n",
    "sns.boxplot(df.bathroomcnt)\n",
    "plt.xlabel('Bathroom count')\n",
    "plt.title('Bathroom Box Plot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bathroomcnt box plots\n",
    "\n",
    "sns.boxplot(df.bathroomcnt, palette = 'husl')\n",
    "plt.xlabel('Bathroom count')\n",
    "plt.title('Bathroom Box Plot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are outliers in properties that have more than 4 bathrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bathroom count distribution, because bathrooms are a discrete variable we will plot the distribution using a histogram\n",
    "\n",
    "plt.hist(df.bathroomcnt, bins = [x*.5 for x in range(0,23)], color = 'red')\n",
    "plt.xlabel('bathroom count')\n",
    "plt.ylabel('count')\n",
    "plt.title('Bathroom Distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.bathroomcnt.value_counts().sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bedroomcnt box plots\n",
    "\n",
    "sns.boxplot(df.bedroomcnt, color = 'blue')\n",
    "plt.xlabel('Bedroom count')\n",
    "plt.title('Bedroom Box Plot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bedroom count distribution, because bedrooms are a discrete variables we will plot the distibution using a histogram.\n",
    "\n",
    "plt.hist(df.bedroomcnt, bins = [x for x in range(0,13)], color = 'red')\n",
    "plt.xlabel('Bedroom count')\n",
    "plt.ylabel('count')\n",
    "plt.title('Bedroom Distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see outliers as well for bedrooms with less than 2 and more than 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For bedroom and bathroom counts most of the values lie between 1 and 6.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculatedfinished squarefeet box plots\n",
    "\n",
    "sns.boxplot(df.calculatedfinishedsquarefeet, color = 'green')\n",
    "plt.xlabel('count')\n",
    "plt.title('Calculated Finished Square Feet Box Plot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculatedfinishedsquarefeet is also a discrete variable.\n",
    "\n",
    "plt.hist(df.calculatedfinishedsquarefeet, bins = 70)\n",
    "plt.xlabel('Square Feet')\n",
    "plt.ylabel('count')\n",
    "plt.title('Square Footage Distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.bedroomcnt.value_counts().sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.calculatedfinishedsquarefeet.value_counts(bins = 70).sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lotsizesquarefeet is a discrete variables. We will split the data into bins to view where most of the properties are at.\n",
    "\n",
    "df.lotsizesquarefeet.value_counts(bins = 100).sort_index().idxmax()\n",
    "plt.hist(df.lotsizesquarefeet, bins = 100)\n",
    "plt.xlabel('Square Feet')\n",
    "plt.ylabel('count')\n",
    "plt.title('Lot Size Distribution')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df.lotsizesquarefeet, bins = 100)\n",
    "plt.xlabel('Square Feet')\n",
    "plt.xscale('log')\n",
    "plt.ylabel('count')\n",
    "plt.title('Lot Size Distribution')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Dictionary:\n",
    "\n",
    "Independent Variables:\n",
    "\n",
    "bathroomcnt: Count of total number of bathrooms on the property. Did not choose to use calculatedbathnbr because this only took into account bathrooms attached to bedrooms.\n",
    "\n",
    "bedroomcnt: Count of total number of bedrooms on the property. Did not choose to use calculatedbathnbr because this only took into account bathrooms attached to bedrooms.\n",
    "\n",
    "calculatedfinishedsquarefeet: Total amount of squarefootage of the property. This does not include lot size. There were other finishedsquare feet columns 12, 13, 50, 15, 6. These other finished square feet columns contained significant amounts of NULL values that would have effected how many data samples we had. Not that having more data is better, we just had another columns with finishedsquarefeet that could represent the living squarefootage of the property.\n",
    "\n",
    "lotsizesquarefeet: Total amount of square feet of the lot. This will be a good value to use because we are looking at property square footage as well that is only the amount of living squarefeet. The lotsize can tell us about the value of the property as well.\n",
    "\n",
    "Target Variable:\n",
    "taxvaluedollarcount: This data calculated both the structuretaxvaluedollarcnt and landtaxvaluedollarcnt. Taxvaluedollarcnt was the similar way to predict how much someone might pay for a property.\n",
    "\n",
    "Index:\n",
    "parcelid: to keep some of the integrity of the data we will use the parcelid number of the properties as index values. This will help in future research of looking at characteristics of specific properties, and how they influence their property value.\n",
    "\n",
    "fips: assigned county of the property\n",
    "    fips locations:\n",
    "        6037 - California - Los Angeles County\n",
    "        6059 - California - Orange County\n",
    "        6111 - California - Venture County\n",
    "\n",
    "Scaling our data:\n",
    "\n",
    "Our independent variables, and target variables are measured in two different ways:\n",
    "    1. Square footage - calculatedfinishedsquarefeet, lotsizesquarefeet\n",
    "    2. Quanity Count - bedroomcnt, bathroomcnt\n",
    "    3. Cost - taxvaluedollarcnt\n",
    "\n",
    "There are some outliers in our data where they have a high bedroom, bathroom, or calculated squarefootage. Because of these values we will use a Robust Scaler. The centering and scaling of the Robust scaler are based on percentiles. This means that the scaling is not influenced by a few number of very large marginal outliers. \n",
    "<https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html>\n",
    "\n",
    "Erroneous or Invalid Data:\n",
    "There was not outstanding erroneous data found in the preperation. We are going to keep properties that have bedrooms, or bathrooms as 0 because they still have a calculatedfinsihedsquarefeet. The squarefootage will drive the values of the property more than the room count. \n",
    "The values that were droped from the dataframe are the null values in calculated finished square feet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use split_scale.py function split_my_data(df) to create train and test dateframes\n",
    "\n",
    "train, test = split_scale.split_my_data(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use split_scale.py function iqr_robust_scaler(train,test) to scale the data so they will be on the same unit scale. We do this because in the prep phase we saw that some variables are measured in squarefeet, and rooms are just a count value.\n",
    "\n",
    "scaler, train_scaled, test_scaled = split_scale.iqr_robust_scaler(train, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at our scaled data\n",
    "\n",
    "train_scaled.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scaled.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_scaled dataframe will be our indepenent variables : bathroomcnt, bedroomcnt,  caluclatedfinishedsquarefeet, lotsizesquarefeet.\n",
    "# y_train_scaled dataframe will be out target variable: taxvaluedollarcnt\n",
    "\n",
    "X_train_scaled = train_scaled[['bathroomcnt', 'bedroomcnt', 'calculatedfinishedsquarefeet', 'lotsizesquarefeet']]\n",
    "y_train_scaled = train_scaled[['taxvaluedollarcnt']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_scaled dataframe will be 20% of our dataframe and reflect the variables in our training datasets.\n",
    "\n",
    "X_test_scaled = test_scaled[['bathroomcnt', 'bedroomcnt', 'calculatedfinishedsquarefeet', 'lotsizesquarefeet']]\n",
    "y_test_scaled = test_scaled[['taxvaluedollarcnt']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Data Exploration***\n",
    "\n",
    "Goal: Address questions posed in the planning brainstorming phase, and any other questions that have come up along the way through visual or statiscal analysis\n",
    "\n",
    "\n",
    "*** Recap from hypothesis above: ***\n",
    "\n",
    "Which independent variables do you think influence the value of a home?\n",
    "\n",
    "Hypothesis:\n",
    "\n",
    "Variables that can influence the value of home are square feet,  and room count(bedroom, and bath). \n",
    "These two variables can have an influence no matter where the property located.\n",
    "\n",
    "The more square feet of a property the more the value of the home will rise.\n",
    "The higher the room count the cost will increase.\n",
    "The total square footage of the property will be a stronger influence than room count\n",
    "\n",
    "When you start to get into specific locations other factors can influence the value of a home such as zip code, school district, distance to destinations (work, home, restaurants, grocery).\n",
    "Also home amenities will influence the value of a property, such as backyards with pools, garage size, hoa.\n",
    "The age of home would also have influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the correlation values between independent variables, and grab the correlation column that compares the variables to our target taxvaluedollarcnt.\n",
    "\n",
    "train_scaled.corr()\n",
    "train_scaled.corr().taxvaluedollarcnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the correlation of a variables with sns.PairGrid\n",
    "\n",
    "g = sns.PairGrid(train_scaled, palette = 'reds')\n",
    "g.map_diag(plt.hist)\n",
    "g.map_offdiag(plt.scatter);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the correlation of variables with sns.heatmap \n",
    "\n",
    "plt.figure(figsize = (8,6))\n",
    "sns.heatmap(train.corr(), cmap = 'Blues', annot = True )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " T test - Is the average taxvaluedollarcnt different for one bedroom vs. three bedrooms?\n",
    " \n",
    " H[0]: The average taxvaluedollarcount of properties with one bedroom or three bedrooms is the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll need to create two seperate datasets that contain the values for taxvaluedollarcnt for properties with one bedroom and properties with two bedrooms\n",
    "\n",
    "bedrooms_1 = train_scaled[train_scaled['bedroomcnt'] == 1]\n",
    "bedrooms_3 = train_scaled[train_scaled['bedroomcnt'] == 3]\n",
    "test_results = ttest_ind(bedrooms_1.taxvaluedollarcnt, bedrooms_3.taxvaluedollarcnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the p value is so small, we reject the null hypothesis. We think there is a significant difference in the average taxvaluedollarcnt of properties with 1 bedroom and properties with 3 bedrooms.\n",
    "# Let's look at those averages:\n",
    "\n",
    "bedrooms_1.taxvaluedollarcnt.mean()\n",
    "bedrooms_3.taxvaluedollarcnt.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T Test - Is the average taxvaluedollarncnt different for one bathroom vs. three bathroom properties?\n",
    "\n",
    "H[0]: The average taxvaluedollarcnt of properties with one bathroom or three bathrooms is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bathrooms_1 = train_scaled[train_scaled['bathroomcnt'] == 1]\n",
    "bathrooms_3 = train_scaled[train_scaled['bathroomcnt'] == 3]\n",
    "test_results = ttest_ind(bathrooms_1.taxvaluedollarcnt, bathrooms_3.taxvaluedollarcnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the p value is so small, we reject the null hypothesis. We think there is a significant difference in the average taxvaluedollarcnt of properties with 1 bathroom and properties with 3 bedrooms.\n",
    "# Let's look at those averages:\n",
    "\n",
    "bathrooms_1.taxvaluedollarcnt.mean()\n",
    "bathrooms_3.taxvaluedollarcnt.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Pearson R - Are bathrooms and bedrooms linearly correlated, and what is the strength of that correlation?\n",
    "H[0]: There is not a linear correlation between number of bathrooms and number of bedrooms for a property.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can pass the two series that contain the values we are looking at to the pearsonr function from scipy's stats module.\n",
    "\n",
    "test_results_pearsonr = pearsonr(train_scaled.bathroomcnt, train_scaled.bedroomcnt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because pearsonr is 0.0 we reject the null hypothesis that there is no linear relationship. The test also tells us the r^2 value of .645"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearson R - Are bathrooms and calculatedfinished square feet linearly correlated, and what is the strength of that correlation?\n",
    "\n",
    "H[0]: There is not a linear correlation between number of bathrooms and calculatedfinished sqaure feet for a property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can pass the two series that contain the values we are looking at to the pearsonr function from scipy's stats module.\n",
    "\n",
    "test_results_pearsonr = pearsonr(train_scaled.bathroomcnt, train_scaled.calculatedfinishedsquarefeet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "because pearsonr is 0.0 we reject the null hypothesis that there is no linear relationship. The test also tells us the r ^2 value of .852 that this the strength of the relationship.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take aways from exploration:\n",
    "Calculated finished square feet is the most correlated to our dependent (target variable) taxvaluedollarcnt. From our hypothesis that our presumption that calculated finished square feet will influence taxvalue dollar count in a positive direction (overall taxvaluedollarcnt increases when calculated finished squarefeet increases)\n",
    "\n",
    "We see that calculated finished square feet correlates with both bedroom, and bathroom count. This makes sense because the amount of liveable square fee will drive the amount of bedrooms, and bathrooms on a property. This finding is going to influence the feature selection. It gives the direction that you could combine bedroom, and bathroom count into one feature. Then use this combined room count feature in the selection process. \n",
    "\n",
    "Don't like that independent variables are correlated with each other so in feature selection process and modeling would want to see which of those give us better prediction values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Feature Selection ***\n",
    "\n",
    "Goal: Create a dataframe(s) with the features to be used to build your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform feature selection using RFE. Use features.py file function optimal_number_of_features to perform recursive feature elimination that will tell us the number of features to use to predict our target variable\n",
    "\n",
    "number_of_features, score = features.optimal_number_of_features(X_train_scaled, y_train_scaled)\n",
    "number_of_features, score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use recursive feature elimination. Use features.py file function optimal features to find out which features should be used\n",
    "\n",
    "selected_features_rfe, X_train_rfe, X_test_rfe = features.optimal_features(X_train_scaled, X_test_scaled, y_train_scaled, number_of_features)\n",
    "selected_features_rfe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Modeling & Evaluation ***\n",
    "Goal: develop a regression model that performs better than using overall average taxvaluedollarcnt as a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prediction dataframes for train and test data. Use model.py function modeling_function to create linear model_1, and compare to baseline prediction of mean taxvaluedollarcnt.\n",
    "predictions_train, predictions_test = model.modeling_function(X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create residual plots for independent variables vs. target variable lotsizesquarefeet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = X_train_scaled[['calculatedfinishedsquarefeet']]\n",
    "x2 = X_train_scaled[['bedroomcnt']]\n",
    "x3 = X_train_scaled[['bathroomcnt']]\n",
    "x4 = X_train_scaled[['lotsizesquarefeet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_train_scaled[['taxvaluedollarcnt']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_residuals(x1,y)\n",
    "model.plot_residuals(x2,y)\n",
    "model.plot_residuals(x3,y)\n",
    "#model.plot_residuals(x4,y)\n",
    "#plt.xscale('log')\n",
    "#plt.xlim(-100,400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot residual of baseline - predicted value is the average taxvaluedollarcnt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_regression(x1,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLS Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train_scaled\n",
    "y = y_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sm.add_constant(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = sm.OLS(y,X).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
